{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:SteelBlue\">Lesson:</span> Building an ML Pipeline</h1>\n",
    "<hr>\n",
    "We've talked about the process of training a model already which is a failry straight forward process when were talking about a single model, with predefined HyperParameters.\n",
    "\n",
    "However, in most real life situations we DO NOT know what models may be the best suited for the task at hand, let alone the appropriate hyperparameters for each model. \n",
    "\n",
    "**The Solution?**\n",
    "\n",
    "We will write a series of loops that use *Cross-Validation* to \"test\" each model, including every combination of relevant hyperparameters â€”for each model, in order to discern which configuration is the most effective.\n",
    "\n",
    "**Relevant topics for this section on Building Pipelines:**\n",
    "1. Dictionaries\n",
    "2. Looping\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import PyData Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NumPy for numerical computing\n",
    "import numpy as np\n",
    "# Pandas for DataFrames\n",
    "import pandas as pd\n",
    "# Pickle for saving model files\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import SKLearn Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import LogisticRegression from sklearn.linear_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# Import RandomForestClassifier and GradientBoostingClassifier from sklearn.ensemble\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "# Import KNeighborsClassifier from sklearn.neighbors \n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries to make Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import train_test_split from sklearn.model_selection\n",
    "from sklearn.model_selection  import train_test_split \n",
    "# import make_pipeline from sklearn.pipeline\n",
    "from sklearn.pipeline import make_pipeline \n",
    "# import StandardScaler from sklearn.preprocessing\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "# import GridSearchCV from sklearn.model_selection\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# import roc_curve and auc from sklearn.metrics\n",
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `read_csv` `pima_diabetes.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./Pima_Indian_Diabetes/data/pima_diabetes.csv')# your code here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `.shape`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 9)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape# your code here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `.head()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()# Your code here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Pregnancies</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.129459</td>\n",
       "      <td>0.141282</td>\n",
       "      <td>-0.081672</td>\n",
       "      <td>-0.073535</td>\n",
       "      <td>0.017683</td>\n",
       "      <td>-0.033523</td>\n",
       "      <td>0.544341</td>\n",
       "      <td>0.221898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Glucose</th>\n",
       "      <td>0.129459</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.152590</td>\n",
       "      <td>0.057328</td>\n",
       "      <td>0.331357</td>\n",
       "      <td>0.221071</td>\n",
       "      <td>0.137337</td>\n",
       "      <td>0.263514</td>\n",
       "      <td>0.466581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BloodPressure</th>\n",
       "      <td>0.141282</td>\n",
       "      <td>0.152590</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.207371</td>\n",
       "      <td>0.088933</td>\n",
       "      <td>0.281805</td>\n",
       "      <td>0.041265</td>\n",
       "      <td>0.239528</td>\n",
       "      <td>0.065068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SkinThickness</th>\n",
       "      <td>-0.081672</td>\n",
       "      <td>0.057328</td>\n",
       "      <td>0.207371</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.436783</td>\n",
       "      <td>0.392573</td>\n",
       "      <td>0.183928</td>\n",
       "      <td>-0.113970</td>\n",
       "      <td>0.074752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Insulin</th>\n",
       "      <td>-0.073535</td>\n",
       "      <td>0.331357</td>\n",
       "      <td>0.088933</td>\n",
       "      <td>0.436783</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.197859</td>\n",
       "      <td>0.185071</td>\n",
       "      <td>-0.042163</td>\n",
       "      <td>0.130548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BMI</th>\n",
       "      <td>0.017683</td>\n",
       "      <td>0.221071</td>\n",
       "      <td>0.281805</td>\n",
       "      <td>0.392573</td>\n",
       "      <td>0.197859</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.140647</td>\n",
       "      <td>0.036242</td>\n",
       "      <td>0.292695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <td>-0.033523</td>\n",
       "      <td>0.137337</td>\n",
       "      <td>0.041265</td>\n",
       "      <td>0.183928</td>\n",
       "      <td>0.185071</td>\n",
       "      <td>0.140647</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.033561</td>\n",
       "      <td>0.173844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Age</th>\n",
       "      <td>0.544341</td>\n",
       "      <td>0.263514</td>\n",
       "      <td>0.239528</td>\n",
       "      <td>-0.113970</td>\n",
       "      <td>-0.042163</td>\n",
       "      <td>0.036242</td>\n",
       "      <td>0.033561</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.238356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Outcome</th>\n",
       "      <td>0.221898</td>\n",
       "      <td>0.466581</td>\n",
       "      <td>0.065068</td>\n",
       "      <td>0.074752</td>\n",
       "      <td>0.130548</td>\n",
       "      <td>0.292695</td>\n",
       "      <td>0.173844</td>\n",
       "      <td>0.238356</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Pregnancies   Glucose  BloodPressure  SkinThickness  \\\n",
       "Pregnancies                  1.000000  0.129459       0.141282      -0.081672   \n",
       "Glucose                      0.129459  1.000000       0.152590       0.057328   \n",
       "BloodPressure                0.141282  0.152590       1.000000       0.207371   \n",
       "SkinThickness               -0.081672  0.057328       0.207371       1.000000   \n",
       "Insulin                     -0.073535  0.331357       0.088933       0.436783   \n",
       "BMI                          0.017683  0.221071       0.281805       0.392573   \n",
       "DiabetesPedigreeFunction    -0.033523  0.137337       0.041265       0.183928   \n",
       "Age                          0.544341  0.263514       0.239528      -0.113970   \n",
       "Outcome                      0.221898  0.466581       0.065068       0.074752   \n",
       "\n",
       "                           Insulin       BMI  DiabetesPedigreeFunction  \\\n",
       "Pregnancies              -0.073535  0.017683                 -0.033523   \n",
       "Glucose                   0.331357  0.221071                  0.137337   \n",
       "BloodPressure             0.088933  0.281805                  0.041265   \n",
       "SkinThickness             0.436783  0.392573                  0.183928   \n",
       "Insulin                   1.000000  0.197859                  0.185071   \n",
       "BMI                       0.197859  1.000000                  0.140647   \n",
       "DiabetesPedigreeFunction  0.185071  0.140647                  1.000000   \n",
       "Age                      -0.042163  0.036242                  0.033561   \n",
       "Outcome                   0.130548  0.292695                  0.173844   \n",
       "\n",
       "                               Age   Outcome  \n",
       "Pregnancies               0.544341  0.221898  \n",
       "Glucose                   0.263514  0.466581  \n",
       "BloodPressure             0.239528  0.065068  \n",
       "SkinThickness            -0.113970  0.074752  \n",
       "Insulin                  -0.042163  0.130548  \n",
       "BMI                       0.036242  0.292695  \n",
       "DiabetesPedigreeFunction  0.033561  0.173844  \n",
       "Age                       1.000000  0.238356  \n",
       "Outcome                   0.238356  1.000000  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initial Data Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Dataset into `X` and `y` sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('Outcome', axis=1)# what should X be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['Outcome']# what should y be?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `shape` of `X`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 8)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape# your code here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `shape` of `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split `X` and `y` into `Train` and `Test` Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We want to split both the `X` and `y` datasets into Train and Test sets. This will result in 4 datasets total.**\n",
    "\n",
    "```(X_train, X_test, y_train, y_test)```\n",
    "\n",
    "> **`X`:** `X_train` and `X_test`\n",
    "\n",
    "> **`y`:** `y_train` and `y_test`\n",
    "\n",
    "The **Train Set** is the set that we will operform further splits on, during the cross-validation step. The motivation for this is to find the best combination of model and hyperparameters, which we can only achieve by running performance metrics on every possible combination of Model and Hyperparameters.\n",
    "\n",
    "The **Test Set** will remain untouched until after the cross-validation step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `train_test_split()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a `train_test_split` with parmeters:\n",
    "```python \n",
    "X\n",
    "y\n",
    "test_size should be 0.2, \n",
    "Stratify by the `Outcome` column, \n",
    "random_state should be 123\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `print()` `shape` of `X_train` and `X_test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(614, 8)\n",
      "(154, 8)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `print()` `shape` of `y_train` and `y_test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(614,)\n",
      "(154,)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both **Train** sets, **X_train and y_train**, should have the **same number of rows**\n",
    "\n",
    "Both **Test** sets, **X_test and y_test**, should also have the **same number of rows**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build Pipeline Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "RandomForestRegressor?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline dictionary can contain as many models as your heart's content. We use this approach to automate the process of fitting a model, predicting labels, and finally testing model accuracy.\n",
    "\n",
    "This will all be done with the SKlearn model_selection function **GridSearchCV()** which expects at least two arguments a **Pipeline** dictionary and a **Hyperparameters** dictionary, with matching keys!!!\n",
    "\n",
    "Let's start with the **Pipeline Dictionary!**\n",
    "\n",
    "The `keys` should contain the model name's â€”as `strings`\n",
    "The `values` should contain pipeline objects.\n",
    ">The `pipeline` objects should contain a normalizer/standardizer, and instantiate a model.\n",
    "\n",
    "<br>\n",
    "\n",
    "`\"model_name\": make_pipeline(scalar()/normalizer(), ModelRegressor()/ModelClassifier())`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border-color:royalblue;background-color:royalblue;height:1px;\">\n",
    "\n",
    "## <span style=\"color:RoyalBlue\">Exercise 1</span> - Add Remaining `Pipelines` to `pipeline_dict`\n",
    "\n",
    "Add the `pipelines` for the three remaining Classifiers: `RandomForrestClassifier`, `GradientBoostClassifier`, and `KNeighborsClassifier`.\n",
    "\n",
    "> 1. All `Pipelines` in `pipeline_dict` should begin with a `StandrardScaler()`\n",
    "> 2. All `ModelInstantiations()` should set a `random_state = 123`, except for `KNeighborsClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StandardScaler - mean = to zero and std = 1 \n",
    "#normalizeScaler -  set range 0-1 mean = .5\n",
    "pipeline_dict = {\n",
    "    'l1': make_pipeline(StandardScaler(), LogisticRegression(penalty= 'l1', random_state= 123)), #l1 == lasso\n",
    "    'l2': make_pipeline(StandardScaler(), LogisticRegression(penalty= 'l2', random_state= 123)), # l2 == ridge\n",
    "    'rf': make_pipeline(StandardScaler(), RandomForestClassifier(random_state= 123)),\n",
    "    'gb': make_pipeline(StandardScaler(), GradientBoostingClassifier(random_state= 123)),\n",
    "    'kn': make_pipeline(StandardScaler(), KNeighborsClassifier())\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check your work! \n",
    "Loope through `pipeline_dict` \n",
    "`keys` should contain a `sklearn.pipeline.Pipeline` object as a matching `value`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l1 : <class 'sklearn.pipeline.Pipeline'>\n",
      "l2 : <class 'sklearn.pipeline.Pipeline'>\n",
      "rf : <class 'sklearn.pipeline.Pipeline'>\n",
      "gb : <class 'sklearn.pipeline.Pipeline'>\n",
      "kn : <class 'sklearn.pipeline.Pipeline'>\n"
     ]
    }
   ],
   "source": [
    "for model_name, pipeline in pipeline_dict.items():\n",
    "    print(model_name, ':', type(pipeline))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Build a Hyperparameters Nested-Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following describes the goal for this section, however we will begin with some exercises to help understand the concept of a nested dictionary!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Final Hyperparameter Dictionary: `hp_dict`**\n",
    "\n",
    "> Every `key` in `hp_dict` will match the keys from `pipeline_dict`.\n",
    "\n",
    "> Every `value` in `hp_dict` will be a dictionary.\n",
    "\n",
    "**Nested Dictionaries inside `hp_dict`**\n",
    "\n",
    "> Every `key` in each nested `dict` will be a different hyperparameter that needs tuning.\n",
    "\n",
    "> Every `value` will hold a range of values that each particular hyperparameter can take on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example of an `hp_dict` with values for 2 different models**\n",
    "\n",
    "*Notice that the `values` of `hp_dict` are surrounded by `{}`, this means that they are themselves dictionaries!*\n",
    "```python \n",
    "hp_dict = {\n",
    "    'l1':{\"logisticregression__C\" : np.linspace(1e-4, 1e4, 10)}\n",
    "}```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border-color:royalblue;background-color:royalblue;height:1px;\">\n",
    "\n",
    "## <span style=\"color:RoyalBlue\">Exercise 2</span> - Performing Action during Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loop through `pipeline_dict` to print out the `type` of object that is associated with the `get_params()` method, for every model \"key.\"**\n",
    "\n",
    "**Remember:** Dictionaries in Python are set up like this:\n",
    "```python \n",
    "some_dict = {\n",
    "    key1 : val1\n",
    "    key2 : val2\n",
    "}```\n",
    "\n",
    "\n",
    "**Hint:** I want you to **`print` every `key`** and the **`type` of object that results from calling the `get_params()` method on every matching `value` of `pipeline_dict`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l1 : <class 'dict'>\n",
      "l2 : <class 'dict'>\n",
      "rf : <class 'dict'>\n",
      "gb : <class 'dict'>\n",
      "kn : <class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "for model_name, pipeline in pipeline_dict.items():\n",
    "    print(model_name, \":\", type(pipeline.get_params()))# Build a loop that checks type of get_params() for ea value in pipeline_dict "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What type of object does `get_params()` return?**\n",
    "\n",
    "dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><hr style=\"border-color:royalblue;background-color:royalblue;height:1px;\">\n",
    "## <span style=\"color:RoyalBlue\">Exercise 3</span> - Nested Dictionary Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Since `get_params()` results in a `dict` object we can use `.items()` to loop through every `key` `value` pair inside of each parameter `dict`!**\n",
    "\n",
    "There is nothing stopping us from nesting a second for loop from inside a for loop!\n",
    "\n",
    "This is one of the harder concepts to understand at first, but once you get it, you'll realize how straightforward the process really is!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l1\n",
      "--------------\n",
      "memory : None\n",
      "steps : [('standardscaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('logisticregression', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l1', random_state=123, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False))]\n",
      "standardscaler : StandardScaler(copy=True, with_mean=True, with_std=True)\n",
      "logisticregression : LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l1', random_state=123, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False)\n",
      "standardscaler__copy : True\n",
      "standardscaler__with_mean : True\n",
      "standardscaler__with_std : True\n",
      "logisticregression__C : 1.0\n",
      "logisticregression__class_weight : None\n",
      "logisticregression__dual : False\n",
      "logisticregression__fit_intercept : True\n",
      "logisticregression__intercept_scaling : 1\n",
      "logisticregression__max_iter : 100\n",
      "logisticregression__multi_class : warn\n",
      "logisticregression__n_jobs : None\n",
      "logisticregression__penalty : l1\n",
      "logisticregression__random_state : 123\n",
      "logisticregression__solver : warn\n",
      "logisticregression__tol : 0.0001\n",
      "logisticregression__verbose : 0\n",
      "logisticregression__warm_start : False\n",
      "\n",
      "l2\n",
      "--------------\n",
      "memory : None\n",
      "steps : [('standardscaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('logisticregression', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=123, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False))]\n",
      "standardscaler : StandardScaler(copy=True, with_mean=True, with_std=True)\n",
      "logisticregression : LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=123, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False)\n",
      "standardscaler__copy : True\n",
      "standardscaler__with_mean : True\n",
      "standardscaler__with_std : True\n",
      "logisticregression__C : 1.0\n",
      "logisticregression__class_weight : None\n",
      "logisticregression__dual : False\n",
      "logisticregression__fit_intercept : True\n",
      "logisticregression__intercept_scaling : 1\n",
      "logisticregression__max_iter : 100\n",
      "logisticregression__multi_class : warn\n",
      "logisticregression__n_jobs : None\n",
      "logisticregression__penalty : l2\n",
      "logisticregression__random_state : 123\n",
      "logisticregression__solver : warn\n",
      "logisticregression__tol : 0.0001\n",
      "logisticregression__verbose : 0\n",
      "logisticregression__warm_start : False\n",
      "\n",
      "rf\n",
      "--------------\n",
      "memory : None\n",
      "steps : [('standardscaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('randomforestclassifier', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
      "            oob_score=False, random_state=123, verbose=0, warm_start=False))]\n",
      "standardscaler : StandardScaler(copy=True, with_mean=True, with_std=True)\n",
      "randomforestclassifier : RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
      "            oob_score=False, random_state=123, verbose=0, warm_start=False)\n",
      "standardscaler__copy : True\n",
      "standardscaler__with_mean : True\n",
      "standardscaler__with_std : True\n",
      "randomforestclassifier__bootstrap : True\n",
      "randomforestclassifier__class_weight : None\n",
      "randomforestclassifier__criterion : gini\n",
      "randomforestclassifier__max_depth : None\n",
      "randomforestclassifier__max_features : auto\n",
      "randomforestclassifier__max_leaf_nodes : None\n",
      "randomforestclassifier__min_impurity_decrease : 0.0\n",
      "randomforestclassifier__min_impurity_split : None\n",
      "randomforestclassifier__min_samples_leaf : 1\n",
      "randomforestclassifier__min_samples_split : 2\n",
      "randomforestclassifier__min_weight_fraction_leaf : 0.0\n",
      "randomforestclassifier__n_estimators : warn\n",
      "randomforestclassifier__n_jobs : None\n",
      "randomforestclassifier__oob_score : False\n",
      "randomforestclassifier__random_state : 123\n",
      "randomforestclassifier__verbose : 0\n",
      "randomforestclassifier__warm_start : False\n",
      "\n",
      "gb\n",
      "--------------\n",
      "memory : None\n",
      "steps : [('standardscaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('gradientboostingclassifier', GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "              max_features=None, max_leaf_nodes=None,\n",
      "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "              min_samples_leaf=1, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              n_iter_no_change=None, presort='auto', random_state=123,\n",
      "              subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
      "              verbose=0, warm_start=False))]\n",
      "standardscaler : StandardScaler(copy=True, with_mean=True, with_std=True)\n",
      "gradientboostingclassifier : GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "              max_features=None, max_leaf_nodes=None,\n",
      "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "              min_samples_leaf=1, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              n_iter_no_change=None, presort='auto', random_state=123,\n",
      "              subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
      "              verbose=0, warm_start=False)\n",
      "standardscaler__copy : True\n",
      "standardscaler__with_mean : True\n",
      "standardscaler__with_std : True\n",
      "gradientboostingclassifier__criterion : friedman_mse\n",
      "gradientboostingclassifier__init : None\n",
      "gradientboostingclassifier__learning_rate : 0.1\n",
      "gradientboostingclassifier__loss : deviance\n",
      "gradientboostingclassifier__max_depth : 3\n",
      "gradientboostingclassifier__max_features : None\n",
      "gradientboostingclassifier__max_leaf_nodes : None\n",
      "gradientboostingclassifier__min_impurity_decrease : 0.0\n",
      "gradientboostingclassifier__min_impurity_split : None\n",
      "gradientboostingclassifier__min_samples_leaf : 1\n",
      "gradientboostingclassifier__min_samples_split : 2\n",
      "gradientboostingclassifier__min_weight_fraction_leaf : 0.0\n",
      "gradientboostingclassifier__n_estimators : 100\n",
      "gradientboostingclassifier__n_iter_no_change : None\n",
      "gradientboostingclassifier__presort : auto\n",
      "gradientboostingclassifier__random_state : 123\n",
      "gradientboostingclassifier__subsample : 1.0\n",
      "gradientboostingclassifier__tol : 0.0001\n",
      "gradientboostingclassifier__validation_fraction : 0.1\n",
      "gradientboostingclassifier__verbose : 0\n",
      "gradientboostingclassifier__warm_start : False\n",
      "\n",
      "kn\n",
      "--------------\n",
      "memory : None\n",
      "steps : [('standardscaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('kneighborsclassifier', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "           weights='uniform'))]\n",
      "standardscaler : StandardScaler(copy=True, with_mean=True, with_std=True)\n",
      "kneighborsclassifier : KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "           weights='uniform')\n",
      "standardscaler__copy : True\n",
      "standardscaler__with_mean : True\n",
      "standardscaler__with_std : True\n",
      "kneighborsclassifier__algorithm : auto\n",
      "kneighborsclassifier__leaf_size : 30\n",
      "kneighborsclassifier__metric : minkowski\n",
      "kneighborsclassifier__metric_params : None\n",
      "kneighborsclassifier__n_jobs : None\n",
      "kneighborsclassifier__n_neighbors : 5\n",
      "kneighborsclassifier__p : 2\n",
      "kneighborsclassifier__weights : uniform\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Build a nested loop that loops through get_params() dict\n",
    "for model_name, pipeline in pipeline_dict.items():\n",
    "    print(model_name)\n",
    "    print('--------------')\n",
    "    for hyper_param, hyper_param_value in pipeline.get_params().items():\n",
    "        print(hyper_param, ':', hyper_param_value)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><hr style=\"border-color:royalblue;background-color:royalblue;height:1px;\">\n",
    "## <span style=\"color:RoyalBlue\">Exercise 4</span> - Making individual HyperParameter Dictionaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters for L1 aka \"LASSO\" Logistic Regression\n",
    "We want to tune:\n",
    "1. C\n",
    "2. should be `10` *equally spaced values* between \n",
    "> `1e-4` or 0.0001 and `1e4` or 10,000\n",
    "3. **Hint**: What numpy *generator* function allows us to create a range of **linearly spaced values**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary for all of the hyperparameters that you want to tune for this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters for L2 aka \"Ridge\" Logistic Regression\n",
    "We want to tune:\n",
    "1. C\n",
    "2. should be `10` *equally spaced values* between \n",
    "> `1e-4` or 0.0001 and `1e4` or 10,000\n",
    "3. **Hint**: What numpy *generator* function allows us to create a range of **linearly spaced values**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary for all of the hyperparameters that you want to tune for this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters for Random Forest Classifier\n",
    "We want to tune:\n",
    "1. n_estimators\n",
    "2. max_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary for all of the hyperparameters that you want to tune for this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters for Gradient Boosting Classifier\n",
    "We want to tune:\n",
    "1. n_estimators\n",
    "2. learning_rate\n",
    "3. max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary for all of the hyperparameters that you want to tune for this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters for KNN Classifier\n",
    "We want to tune:\n",
    "1. n_neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary for all of the hyperparameters that you want to tune for this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><hr style=\"border-color:royalblue;background-color:royalblue;height:1px;\">\n",
    "## <span style=\"color:RoyalBlue\">Exercise 5a</span> - Assemble `hp_dict`!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct the final `hp_dict` with similar naming convention to `pipeline_dict`\n",
    "ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_dict = {\n",
    "    'l1' : l1_hyperparameters,\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:RoyalBlue\">Exercise 5b</span> - Loop throu `hp_dict` to verify work!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a for loop that prints out each `key` and `value` in `hp_dict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your work here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><hr style=\"border-color:royalblue;background-color:royalblue;height:1px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have both a `Pipeline_dict` and a `hp_dict` we can build the loop that will handle cross-validation for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><hr style=\"border-color:royalblue;background-color:royalblue;height:1px;\">\n",
    "## <span style=\"color:RoyalBlue\">Exercise 6</span> - Write a loop that uses `GridSearchCV()` to fit models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty dictionary called fitted_models\n",
    "\n",
    "# Loop through model pipelines, tuning each one and saving it to fitted_models\n",
    "    \n",
    "    # Create cross-validation object from pipeline and hyperparameters\n",
    "    \n",
    "    # Fit model on X_train, y_train\n",
    "    \n",
    "    # Store model in fitted_models[name] \n",
    "    \n",
    "    # Print '{name} has been fitted'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluate metrics\n",
    "\n",
    "Finally, it's time to evaluate our models and pick the best one.\n",
    "\n",
    "<br>\n",
    "\n",
    "**First, display the <code style=\"color:steelblue\">best\\_score_</code> attribute for each fitted model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><hr style=\"border-color:royalblue;background-color:royalblue;height:1px;\">\n",
    "## <span style=\"color:RoyalBlue\">Exercise 7</span> - Access `best_score_` attribute for each cross-validation object in `fitted_models`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hint: this is what it looks like to access the `best_score_` attribute for a single member of `fitted_models`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_models['l1'].best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop thorugh fitted_models and check best_score_ for each fitted_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><hr style=\"border-color:royalblue;background-color:royalblue;height:1px;\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
